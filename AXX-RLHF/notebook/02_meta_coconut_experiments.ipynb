{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# 1. 加载本地模型\n",
    "model_path = \"Qwen2.5-0.5B-Instruct/\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"cuda:0\")\n",
    "\n",
    "# 2. 定义提取隐藏状态的函数\n",
    "def extract_hidden_state(text, model, tokenizer):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # 设置output_hidden_states=True来获取隐藏状态\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_hidden_states=True)\n",
    "    \n",
    "    # 获取最后一层的最后一个token的隐藏状态\n",
    "    last_hidden_state = outputs.hidden_states[-1][0, -1, :]\n",
    "    \n",
    "    return last_hidden_state.cpu()\n",
    "\n",
    "# # 3. 文本分类示例\n",
    "# def text_classification_example():\n",
    "#     # 准备数据\n",
    "#     texts = [\"这是一个积极的评论\", \"这个产品太糟糕了\", \"价格合理，质量一般\"]\n",
    "#     labels = [1, 0, 1]  # 1=积极, 0=消极\n",
    "    \n",
    "#     # 提取隐藏状态\n",
    "#     hidden_states = [extract_hidden_state(text, model, tokenizer) for text in texts]\n",
    "#     hidden_states_tensor = torch.stack(hidden_states)\n",
    "    \n",
    "#     # 简单线性分类器\n",
    "#     classifier = torch.nn.Linear(hidden_states_tensor.shape[1], 2)\n",
    "    \n",
    "#     # 训练分类器...\n",
    "#     # 使用分类器进行预测...\n",
    "def extract_sentence_embedding(text, model, tokenizer):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_hidden_states=True)\n",
    "    \n",
    "    # 获取最后一层的所有token的隐藏状态\n",
    "    last_hidden_states = outputs.hidden_states[-1][0]\n",
    "    \n",
    "    # 忽略特殊token (如[CLS], [SEP]等)，计算平均值\n",
    "    # 对于Qwen模型，可能需要根据具体情况调整\n",
    "    input_ids = inputs.input_ids[0]\n",
    "    mask = torch.ones_like(input_ids).float()\n",
    "    special_tokens = tokenizer.all_special_ids\n",
    "    for special_id in special_tokens:\n",
    "        mask = mask * (input_ids != special_id).float()\n",
    "    \n",
    "    # 应用mask并计算平均\n",
    "    masked_states = last_hidden_states * mask.unsqueeze(-1)\n",
    "    sum_embeddings = torch.sum(masked_states, dim=0)\n",
    "    sum_mask = torch.sum(mask).item()\n",
    "    \n",
    "    # 避免除以0\n",
    "    if sum_mask > 0:\n",
    "        mean_embedding = sum_embeddings / sum_mask\n",
    "    else:\n",
    "        mean_embedding = last_hidden_states.mean(dim=0)\n",
    "    \n",
    "    return mean_embedding.cpu()\n",
    "\n",
    "# 4. 相似度匹配示例\n",
    "def similarity_matching_example():\n",
    "    query = \"我肯定。我我说话，你能听懂吗？\"\n",
    "    candidates = [\n",
    "        \"这个可以。缓一缓再还吗？\",\n",
    "        \"你什么意思啊？\",\n",
    "        \"你是不是听不懂我说话\",\n",
    "        \"我是说。\",\n",
    "        \"你能听到我说话吗\",\n",
    "        \"我听不懂。\"\n",
    "    ]\n",
    "    \n",
    "    # 提取查询的隐藏状态\n",
    "    query_state = extract_sentence_embedding(query, model, tokenizer)\n",
    "    \n",
    "    # 提取候选的隐藏状态\n",
    "    candidate_states = [extract_sentence_embedding(c, model, tokenizer) for c in candidates]\n",
    "    \n",
    "    # 计算余弦相似度\n",
    "    similarities = [torch.nn.functional.cosine_similarity(query_state, c_state, dim=0) \n",
    "                    for c_state in candidate_states]\n",
    "    \n",
    "    # 输出结果\n",
    "    for i, sim in enumerate(similarities):\n",
    "        print(f\"与'{candidates[i]}'的相似度: {sim.item():.4f}\")\n",
    "\n",
    "def extract_sentence_embedding(text, model, tokenizer):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_hidden_states=True)\n",
    "    \n",
    "    # 获取最后一层的所有token的隐藏状态\n",
    "    last_hidden_states = outputs.hidden_states[-1][0]\n",
    "    \n",
    "    # 忽略特殊token (如[CLS], [SEP]等)，计算平均值\n",
    "    # 对于Qwen模型，可能需要根据具体情况调整\n",
    "    input_ids = inputs.input_ids[0]\n",
    "    mask = torch.ones_like(input_ids).float()\n",
    "    special_tokens = tokenizer.all_special_ids\n",
    "    for special_id in special_tokens:\n",
    "        mask = mask * (input_ids != special_id).float()\n",
    "    \n",
    "    # 应用mask并计算平均\n",
    "    masked_states = last_hidden_states * mask.unsqueeze(-1)\n",
    "    sum_embeddings = torch.sum(masked_states, dim=0)\n",
    "    sum_mask = torch.sum(mask).item()\n",
    "    \n",
    "    # 避免除以0\n",
    "    if sum_mask > 0:\n",
    "        mean_embedding = sum_embeddings / sum_mask\n",
    "    else:\n",
    "        mean_embedding = last_hidden_states.mean(dim=0)\n",
    "    \n",
    "    return mean_embedding.cpu()\n",
    "\n",
    "similarity_matching_example()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
