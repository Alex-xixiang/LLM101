{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRPO相关推导实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 优势函数部分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在GRPO中，优势函数通过真实环境中的一组奖励reward计算得到，而不是通过对价值估计来计算得到的。\n",
    "$$ A_i = \\frac{r_i - mean({r_1, r2,...,r_G})}{std({r1, r2,...,r_G})}  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 代码实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grpo_adv(rewards):\n",
    "    \"\"\"\n",
    "    计算GRPO中的优势函数\n",
    "    \n",
    "    Args:\n",
    "        rewards: 一组奖励值，形状为 [batch_size]\n",
    "        \n",
    "    Returns:\n",
    "        advantages: 计算得到的优势函数值，形状与rewards相同\n",
    "    \"\"\"\n",
    "    \n",
    "    # 计算rewards的均值和标准差\n",
    "    mean_rewards = torch.mean(rewards)\n",
    "    std_rewards = torch.std(rewards)\n",
    "    \n",
    "    # 防止除以零\n",
    "    if std_rewards == 0:\n",
    "        return torch.zeros_like(rewards)\n",
    "    \n",
    "    # 计算优势函数\n",
    "    advantages = (rewards - mean_rewards) / (std_rewards + 1e-8)\n",
    "    \n",
    "    return advantages\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 0., 1., 1., 0.])\n",
      "tensor([-0.9129,  0.9129, -0.9129,  0.9129,  0.9129, -0.9129])\n"
     ]
    }
   ],
   "source": [
    "rewards = torch.tensor([0, 1, 0, 1, 1, 0], dtype=torch.float)\n",
    "# convert to float\n",
    "print(rewards)\n",
    "adv_rst = grpo_adv(rewards)\n",
    "print(adv_rst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. KL散度部分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GRPO采用KL散度来计算两个概率分布之间的差异\n",
    "$$ \\text{D}_{\\text{KL}} \\left( \\pi_{\\theta} \\left\\| \\pi_{\\text{ref}} \\right\\| \\right) = \\frac{\\pi_{\\text{ref}}(o_i|q)}{\\pi_{\\theta}(o_i|q)} - \\log \\frac{\\pi_{\\text{ref}}(o_i|q)}{\\pi_{\\theta}(o_i|q)} - 1 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 代码实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grpo_kl(pi_logprobs, pi_ref_logprobs):\n",
    "    \"\"\"\n",
    "    计算KL散度\n",
    "    \n",
    "    Args:\n",
    "    pi_logprobs: 当前策略的对数概率\n",
    "    pi_ref_logprobs: 参考策略的对数概率\n",
    "    \n",
    "    Return:\n",
    "    KL散度值\n",
    "    \"\"\"\n",
    "    # 计算概率比值的对数: log(pi_ref/pi) = log(pi_ref) - log(pi)\n",
    "    log_ratio = pi_ref_logprobs - pi_logprobs\n",
    "    \n",
    "    # 计算概率比值: pi_ref/pi = exp(log(pi_ref/pi))\n",
    "    ratio = torch.exp(log_ratio)\n",
    "    \n",
    "    # 计算KL散度: ratio - log(ratio) - 1\n",
    "    kl = ratio - log_ratio - 1.0\n",
    "    \n",
    "    return kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前策略的对数概率: tensor([-0.5775,  0.1163, -1.1137, -0.2934, -0.9523])\n",
      "参考策略的对数概率: tensor([ 1.2962, -0.7097,  1.6133, -0.4940,  0.9184])\n",
      "计算的KL散度: tensor([ 3.6390,  0.2638, 11.5589,  0.0188,  3.6218])\n",
      "KL散度平均值: 3.8204777240753174\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 创建随机的对数概率分布\n",
    "batch_size = 5\n",
    "pi_logprobs = torch.randn(batch_size)\n",
    "pi_ref_logprobs = torch.randn(batch_size)\n",
    "\n",
    "# 计算KL散度\n",
    "kl = grpo_kl(pi_logprobs, pi_ref_logprobs)\n",
    "\n",
    "# 打印结果\n",
    "print(\"当前策略的对数概率:\", pi_logprobs)\n",
    "print(\"参考策略的对数概率:\", pi_ref_logprobs)\n",
    "print(\"计算的KL散度:\", kl)\n",
    "print(\"KL散度平均值:\", kl.mean().item())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. GRPO Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{1}{G} \\sum_{i=1}^G \\left( \\min \\left( \\frac{\\pi_{\\theta}(o_i|q)}{\\pi_{\\theta_{\\text{old}}}(o_i|q)} A_i, \\text{clip} \\left( \\frac{\\pi_{\\theta}(o_i|q)}{\\pi_{\\theta_{\\text{old}}}(o_i|q)}, 1 - \\varepsilon, 1 + \\varepsilon \\right) A_i \\right) - \\beta \\text{D}_{\\text{KL}} \\left( \\pi_{\\theta} \\left\\| \\pi_{\\text{ref}} \\right\\| \\right) \\right) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这部分是目标函数的表达式，包含一个求和项，其中 $G$ 是总的观测数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 对于每个观测 $o_i$，计算当前策略 $\\pi_{\\theta}(o_i|q)$ 与旧策略 $\\pi_{\\theta_{\\text{old}}}(o_i|q)$ 的比率，并乘以对应的优势函数 $A_i$。\n",
    "- 使用 $\\text{clip}$ 函数对比率进行限制，防止其偏离 1 过多。\n",
    "- 最后，减去一个 $\\beta$ 倍的 KL 散度 $\\text{D}_{\\text{KL}} \\left( \\pi_{\\theta} \\left\\| \\pi_{\\text{ref}} \\right\\| \\right)$，这部分用于控制新策略与参考策略 $\\pi_{\\text{ref}}$ 的相似性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1 代码实现（手推）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grpo_loss(pi_logprob, \n",
    "              pi_old_logprob, \n",
    "              pi_ref_logprob, \n",
    "              advantage, \n",
    "              input_len, \n",
    "              len_oi):\n",
    "    \"\"\"\n",
    "    计算GRPO损失\n",
    "    \n",
    "    Args:\n",
    "        pi_logprob: 当前策略的对数概率 [batch_size, input_len]\n",
    "        pi_old_logprob: 旧策略的对数概率 [batch_size, input_len]\n",
    "        pi_ref_logprob: 参考策略的对数概率 [batch_size, input_len]\n",
    "        advantage: 优势函数 [batch_size]\n",
    "        input_len: 输入长度\n",
    "        len_oi: 观测长度 [batch_size]\n",
    "    Returns:\n",
    "        loss: GRPO损失 [batch_size]\n",
    "    \"\"\"\n",
    "    epsilon = 0.2\n",
    "    beta = 0.3\n",
    "    group_size = 2\n",
    "    \n",
    "    # 创建mask并处理无效样本\n",
    "    mask = torch.ones_like(pi_logprob)\n",
    "    mask[torch.where(len_oi == 0)] = 0\n",
    "    \n",
    "    # 计算概率比率\n",
    "    ratio = torch.exp(pi_logprob - pi_old_logprob)\n",
    "    ratio_clip = torch.clamp(ratio, 1 - epsilon, 1 + epsilon)\n",
    "    \n",
    "    # 调整advantage维度以匹配输入\n",
    "    advantage = advantage.unsqueeze(1).expand(-1, input_len)\n",
    "    \n",
    "    # 计算policy gradient\n",
    "    policy_gradient = torch.min(ratio * advantage, ratio_clip * advantage)\n",
    "    \n",
    "    # 计算KL散度\n",
    "    kl = grpo_kl(pi_logprob, pi_ref_logprob)\n",
    "    \n",
    "    # 先应用mask\n",
    "    masked_loss = (policy_gradient - beta * kl) * mask\n",
    "    \n",
    "    # 计算每个样本的有效token数量（防止除零）\n",
    "    valid_tokens = torch.clamp(len_oi, min=1).unsqueeze(1)\n",
    "    \n",
    "    # 计算损失\n",
    "    loss = (-1 / group_size) * masked_loss.sum(dim=1) / valid_tokens.squeeze(1)\n",
    "    \n",
    "    return loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
